version: '3.8'

services:
  jobbergate-api:
    build:
      context: ../jobbergate-api
      dockerfile: Dockerfile-dev
    networks:
      - jobbergate-net
    volumes:
      - ../jobbergate-api/jobbergate_api/:/app/jobbergate_api/
      - ../jobbergate-api/dev_tools/:/app/dev_tools/
      - ../jobbergate-api/alembic/:/app/alembic/
    environment:
      - DATABASE_HOST=db
      - DATABASE_USER=compose-db-user
      - DATABASE_PSWD=compose-db-pswd
      - DATABASE_NAME=compose-db-name
      - DATABASE_PORT=5432
      - S3_ENDPOINT_URL=http://minio:9000
      - S3_BUCKET_NAME=jobbergate-resources
      - AWS_ACCESS_KEY_ID=compose-s3-key
      - AWS_SECRET_ACCESS_KEY=compose-s3-secret
      - ARMASEC_DOMAIN=keycloak.local:8080/realms/jobbergate-local
      - ARMASEC_AUDIENCE=https://local.omnivector.solutions
      - ARMASEC_DEBUG=false
      - ARMASEC_USE_HTTPS=false
      - LOG_LEVEL=DEBUG
    ports:
      - 8000:80
    healthcheck:
      test: curl --fail http://localhost:80/jobbergate/health || exit 1
      interval: 5s
      retries: 10
      timeout: 5s
    depends_on:
      db:
        condition: service_healthy
      keycloak.local:
        condition: service_healthy
      minio-create-bucket:
        condition: service_completed_successfully

  jobbergate-api-qa:
    build:
      context: ../jobbergate-api
      dockerfile: Dockerfile-dev
    profiles: ["dev"]
    networks:
      - jobbergate-net
    environment:
      - TEST_DATABASE_HOST=test-db
      - TEST_DATABASE_PORT=5432
      - TEST_S3_ENDPOINT_URL=http://minio:9000
      - TEST_LOG_LEVEL=CRITICAL
    volumes:
      - ../jobbergate-api/jobbergate_api/:/app/jobbergate_api/
      - ../jobbergate-api/dev_tools/:/app/dev_tools/
      - ../jobbergate-api/tests/:/app/tests/
    entrypoint: ""
    command: make qa
    depends_on:
      test-db:
        condition: service_healthy
      minio-create-bucket:
        condition: service_completed_successfully

  jobbergate-cli:
    build:
      context: ../jobbergate-cli
      dockerfile: Dockerfile
    networks:
      - jobbergate-net
    volumes:
      - ../jobbergate-cli/jobbergate_cli/:/app/jobbergate_cli/
      - ../examples/simple-application/:/simple-example/
      - ../examples/motorbike-application/:/motorbike-example/
      - ./etc/run-motorbike.py:/app/run-motorbike
      - ./slurm-fake-nfs:/nfs
      - jobbergate-cli-cache:/cache/
    environment:
      - ARMADA_API_BASE=http://jobbergate-api:80
      - OIDC_DOMAIN=keycloak.local:8080/realms/jobbergate-local
      - OIDC_AUDIENCE=https://local.omnivector.solutions
      - OIDC_CLIENT_ID=cli
      - OIDC_USE_HTTPS=false
      - JOBBERGATE_CACHE_DIR=/cache
      - DEFAULT_CLUSTER_NAME=local-slurm

  jobbergate-agent:
    build:
      context: ../jobbergate-agent/
      dockerfile: Dockerfile.dev
    networks:
      - jobbergate-net
    volumes:
      - ../jobbergate-agent/jobbergate_agent/:/app/jobbergate_agent
      - ../jobbergate-core/:/jobbergate-core
      - jobbergate-agent-cache:/cache/
      - ./slurm-work-dir:/slurm-work-dir/
    command: ["jg-run"]
    environment:
      - JOBBERGATE_AGENT_BASE_SLURMRESTD_URL=http://slurmrestd:6820
      - JOBBERGATE_AGENT_X_SLURM_USER_NAME=local-user
      - JOBBERGATE_AGENT_DEFAULT_SLURM_WORK_DIR=/slurm-work-dir
      - JOBBERGATE_AGENT_BASE_API_URL=http://jobbergate-api:80
      - JOBBERGATE_AGENT_OIDC_DOMAIN=keycloak.local:8080/realms/jobbergate-local
      - JOBBERGATE_AGENT_OIDC_AUDIENCE=https://local.omnivector.solutions
      - JOBBERGATE_AGENT_OIDC_CLIENT_ID=local-slurm
      - JOBBERGATE_AGENT_OIDC_CLIENT_SECRET=SVkaJ2f9xeYfOVzQPHXYyiwr12za4xGF
      - JOBBERGATE_AGENT_OIDC_USE_HTTPS=false
      - JOBBERGATE_AGENT_CACHE_DIR=/cache
      - JOBBERGATE_AGENT_SLURMRESTD_JWT_KEY_STRING=${JWT_SECRET:-supersecret}
      - JOBBERGATE_AGENT_TASK_JOBS_INTERVAL_SECONDS=15
      - JOBBERGATE_AGENT_SLURM_RESTD_VERSION=v0.0.37
    depends_on:
      jobbergate-api:
        condition: service_healthy

  db:
    image: postgres
    restart: always
    networks:
      - jobbergate-net
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    environment:
      - POSTGRES_PASSWORD=compose-db-pswd
      - POSTGRES_USER=compose-db-user
      - POSTGRES_DB=compose-db-name
    ports:
      - 5432:5432
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5

  test-db:
    image: postgres
    restart: always
    networks:
      - jobbergate-net
    volumes:
      - ./etc/docker-postgres-multiple-databases:/docker-entrypoint-initdb.d
    environment:
      - POSTGRES_PASSWORD=test-pswd
      - POSTGRES_USER=test-user
      - POSTGRES_MULTIPLE_DATABASES="test-db","alt-test-db"
    ports:
      - 5433:5432
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5

  minio:
    image: minio/minio:RELEASE.2023-10-25T06-33-25Z
    networks:
      - jobbergate-net
    volumes:
      - minio_data:/data
      - ./minio-fake-s3:/export
    ports:
      - 9000:9000
      - 9001:9001
    environment:
      - MINIO_ROOT_USER=compose-s3-key
      - MINIO_ROOT_PASSWORD=compose-s3-secret
    healthcheck:
      test: curl --fail http://localhost:9000/minio/health/live || exit 1
      interval: 5s
      retries: 10
      timeout: 5s
    command: ["server", "--compat", "--console-address", ':9001', "/data"]

  minio-create-bucket:
    image: minio/mc
    networks:
      - jobbergate-net
    volumes:
      - ./etc/create-bucket.sh:/create-bucket.sh
    entrypoint: /create-bucket.sh
    depends_on:
      minio:
        condition: service_healthy

  keycloak.local:
    image: keycloak/keycloak:18.0.0
    restart: always
    networks:
      - jobbergate-net
    volumes:
      - kc-realm-files:/opt/keycloak/data/import/
      - ./etc/jobbergate-local.json:/opt/keycloak/data/import/jobbergate-local.json
    environment:
      - KEYCLOAK_ADMIN=admin
      - KEYCLOAK_ADMIN_PASSWORD=admin
      - KC_HEALTH_ENABLED=true
    command:
      - "start-dev"
      - "--import-realm"
    ports:
      - 8080:8080
    healthcheck:
      test: curl --fail http://localhost:8080/health/ready || exit 1
      interval: 5s
      retries: 10
      timeout: 5s

  mysql:
    image: mysql:5.7
    networks:
      - jobbergate-net
    hostname: mysql
    container_name: mysql
    environment:
      - MYSQL_RANDOM_ROOT_PASSWORD=yes
      - MYSQL_DATABASE=slurm_acct_db
      - MYSQL_USER=slurm
      - MYSQL_PASSWORD=password
    volumes:
      - var_lib_mysql:/var/lib/mysql

  slurmctld:
    build:
      context: .
      dockerfile: Dockerfile-slurm
      args:
        - JWT_SECRET=${JWT_SECRET:-supersecret}
    image: slurm-docker-cluster
    networks:
      - jobbergate-net
    command: ["slurmctld"]
    container_name: slurmctld
    hostname: slurmctld
    volumes:
      - etc_munge:/etc/munge
      - var_log_slurm:/var/log/slurm
      - ./slurm-fake-nfs:/nfs
      - ./slurm-work-dir:/slurm-work-dir
    expose:
      - "6817"

  slurmdbd:
    build:
      context: .
      dockerfile: Dockerfile-slurm
    image: slurm-docker-cluster
    networks:
      - jobbergate-net
    command: ["slurmdbd"]
    container_name: slurmdbd
    hostname: slurmdbd
    volumes:
      - etc_munge:/etc/munge
      - var_log_slurm:/var/log/slurm
      - ./slurm-fake-nfs:/nfs
      - ./slurm-work-dir:/slurm-work-dir
    expose:
      - "6819"
    depends_on:
      - slurmctld
      - mysql

  c1:
    privileged: true
    build:
      context: .
      dockerfile: Dockerfile-slurm
    image: slurm-docker-cluster
    networks:
      - jobbergate-net
    command: ["slurmd"]
    hostname: c1
    container_name: c1
    volumes:
      - etc_munge:/etc/munge
      - var_log_slurm:/var/log/slurm
      - ./slurm-fake-nfs:/nfs
      - ./slurm-work-dir:/slurm-work-dir
    expose:
      - "6818"
    depends_on:
      - slurmctld

  c2:
    privileged: true
    build:
      context: .
      dockerfile: Dockerfile-slurm
    image: slurm-docker-cluster
    networks:
      - jobbergate-net
    command: ["slurmd"]
    hostname: c2
    container_name: c2
    volumes:
      - etc_munge:/etc/munge
      - var_log_slurm:/var/log/slurm
      - ./slurm-fake-nfs:/nfs
      - ./slurm-work-dir:/slurm-work-dir
    expose:
      - "6818"
    depends_on:
      - slurmctld

  slurmrestd:
    build:
      context: .
      dockerfile: Dockerfile-slurm
    image: slurm-docker-cluster
    networks:
      - jobbergate-net
    command: ["slurmrestd"]
    environment:
      - SLURMRESTD_SECURITY=disable_unshare_files,disable_unshare_sysv,disable_user_check
      - SLURM_JWT=daemon
    container_name: slurmrestd
    hostname: slurmrestd
    volumes:
      - etc_munge:/etc/munge
      - var_log_slurm:/var/log/slurm
      - ./slurm-fake-nfs:/nfs
      - ./slurm-work-dir:/slurm-work-dir
    expose:
      - "6820"
    ports:
      - 6820:6820
    depends_on:
      - slurmctld

volumes:
  postgres_data:
  minio_data:
  kc-realm-files:
  etc_munge:
  var_lib_mysql:
  var_log_slurm:
  jobbergate-agent-cache:
  jobbergate-cli-cache:

networks:
  jobbergate-net:
    driver: bridge
